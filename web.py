# -*- coding: utf-8 -*-
"""web.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Wqn3L3or0Mk7hnHK6sjyqTuGO84hoX9o
"""

from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import joblib
import io
import traceback
import re
import os

app = FastAPI(
    title="Orchid Species Classifier API",
    description="Predict orchid species from thermal-fluorescence data using Decision Tree",
    version="1.0.1"
)

# Allow all origins (adjust for production)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 1. Load Model ---
MODEL_PATH = "orchid_decision_tree_v1.pkl"

model = None
le = None
class_labels = None

if os.path.exists(MODEL_PATH):
    try:
        artifacts = joblib.load(MODEL_PATH)
        model = artifacts["model"]
        le = artifacts["label_encoder"]
        class_labels = model.classes_
        print("‚úÖ Model loaded successfully.")
        print("Number of classes:", len(class_labels))
    except Exception as e:
        print(f"‚ùå Model loading failed: {e}")
        model = None
        le = None
else:
    print(f"‚ö†Ô∏è Model file not found: {MODEL_PATH}")

@app.get("/")
def home():
    return {
        "message": "Orchid Species Prediction API is running!",
        "model_loaded": model is not None
    }

# --- 2. Feature Extraction (identical to Colab) ---
def extract_peak_features(t, f):
    mask = ~np.isnan(t) & ~np.isnan(f)
    t = np.asarray(t[mask])
    f = np.asarray(f[mask])
    if len(t) < 3:
        return np.nan, np.nan, np.nan, np.nan

    sort_idx = np.argsort(t)
    t = t[sort_idx]
    f = f[sort_idx]

    peak_idx = np.argmax(f)
    F_peak = float(f[peak_idx])
    T_peak = float(t[peak_idx])

    # Use np.trapz to match Colab exactly
    area = float(np.trapz(f, t))

    half = F_peak / 2.0
    above_half = np.where(f >= half)[0]
    if len(above_half) >= 2:
        width = float(t[above_half[-1]] - t[above_half[0]])
    else:
        width = np.nan
    return T_peak, F_peak, width, area

# --- 3. T/F Pair Detection (identical to Colab) ---
def find_tf_pairs(columns):
    pairs = []
    for col in columns:
        m = re.match(r"^(.*)T(\d+)$", col)
        if m:
            prefix, rep = m.group(1), m.group(2)
            t_col = col
            f_col = f"{prefix}F{rep}"
            if f_col in columns:
                pairs.append((t_col, f_col))
    return pairs

# --- 4. Main Prediction Endpoint ---
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        if model is None:
            return {"error": "Model not loaded", "results": []}

        # Read uploaded Excel file
        contents = await file.read()
        df = pd.read_excel(io.BytesIO(contents))
        print(f"üì• Received file with shape: {df.shape}")

        # Find T/F pairs
        tf_pairs = find_tf_pairs(df.columns.tolist())
        if not tf_pairs:
            return {"error": "No valid T/F column pairs found", "results": []}

        # Extract features from ALL pairs
        all_features = []
        for t_col, f_col in tf_pairs:
            t_vals = pd.to_numeric(df[t_col], errors='coerce').values
            f_vals = pd.to_numeric(df[f_col], errors='coerce').values
            T_peak, F_peak, width, area = extract_peak_features(t_vals, f_vals)
            if not np.isnan(T_peak):
                all_features.append([T_peak, F_peak, width, area])

        if not all_features:
            return {"error": "No valid features extracted", "results": []}

        X_all = np.array(all_features)
        print(f"üß† Input shape: {X_all.shape}")

        # Predict
        probs_all = model.predict_proba(X_all)
        avg_probs = np.mean(probs_all, axis=0)

        # Get top 4
        top_indices = np.argsort(avg_probs)[::-1][:4]

        top_4_data = []
        top_4_sum = 0
        max_score = 0

        for rank, idx in enumerate(top_indices, start=1):
            label_code = class_labels[idx]
            try:
                species_name = le.inverse_transform([label_code])[0]
            except Exception:
                species_name = "Unknown"

            confidence = int(round(avg_probs[idx] * 100))
            if rank == 1:
                max_score = confidence

            top_4_sum += confidence

            display_text = f"{rank}. {species_name} {confidence}%"
            top_4_data.append({
                "rank": rank,
                "species": display_text,
                "confidence": confidence
            })

        # üîπ ‡πÄ‡∏û‡∏¥‡πà‡∏° "Other Species" ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠
        others_percent = max(0, 100 - top_4_sum)
        if others_percent > 0:
            others_text = f"5. Other Species {others_percent}%"
            top_4_data.append({
                "rank": 5,
                "species": others_text,
                "confidence": others_percent
            })

        result = {
            "filename": file.filename,
            "group_real_name": "Combined Prediction",
            "top_4_details": top_4_data,  # ‡∏à‡∏£‡∏¥‡∏á‡πÜ ‡∏ï‡∏≠‡∏ô‡∏ô‡∏µ‡πâ‡∏°‡∏µ 5 ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£
            "sort_score": max_score
        }

        return {"results": [result]}

    except Exception as e:
        print("‚ùå Error during prediction:")
        print(traceback.format_exc())
        return {"error": str(e), "results": []}