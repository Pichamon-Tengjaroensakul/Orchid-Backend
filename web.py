# -*- coding: utf-8 -*-
"""web.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jJKSdq8rTp3n7Qx3NYIkaOAjJ8xovQzW
"""

from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from typing import List
import uvicorn
import pandas as pd
import numpy as np
import joblib
import os
import re
import io
import base64
import matplotlib
matplotlib.use('Agg')
import matplotlib.pyplot as plt

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ==========================================
# 1. SETUP
# ==========================================
MODEL_FILENAME = 'orchid_decision_tree_v1.pkl'
REF_DATA_FILENAME = 'PROJECT_DATA.xlsx'

MODEL_PATH = os.path.join(os.path.dirname(__file__), MODEL_FILENAME)
REF_PATH = os.path.join(os.path.dirname(__file__), REF_DATA_FILENAME)

model_data = None
ref_df = None

# Load Model
try:
    if os.path.exists(MODEL_PATH):
        model_data = joblib.load(MODEL_PATH)
        print(f"‚úÖ Model Loaded: {MODEL_FILENAME}")
    else:
        print(f"‚ùå Model Not Found: {MODEL_PATH}")
except Exception as e:
    print(f"‚ùå Error loading model: {e}")

# Load Reference Data
try:
    if os.path.exists(REF_PATH):
        if REF_DATA_FILENAME.endswith('.csv'):
            ref_df = pd.read_csv(REF_PATH)
        else:
            ref_df = pd.read_excel(REF_PATH)
        print(f"‚úÖ Reference Data Loaded: {REF_DATA_FILENAME} ({len(ref_df)} rows)")
        # Clean column names (strip spaces)
        ref_df.columns = ref_df.columns.str.strip()
    else:
        print(f"‚ö†Ô∏è Reference Data Not Found at: {REF_PATH}")
except Exception as e:
    print(f"‚ö†Ô∏è Error loading reference data: {e}")

# ==========================================
# 2. HELPER FUNCTIONS
# ==========================================
def extract_peak_features(t, f):
    try:
        mask = ~np.isnan(t) & ~np.isnan(f)
        t, f = np.asarray(t[mask]), np.asarray(f[mask])
        if len(t) < 3: return np.nan, np.nan, np.nan, np.nan

        sort_idx = np.argsort(t)
        t, f = t[sort_idx], f[sort_idx]

        peak_idx = np.argmax(f)
        F_peak = float(f[peak_idx])
        T_peak = float(t[peak_idx])

        if hasattr(np, 'trapezoid'): area = float(np.trapezoid(f, x=t))
        else: area = float(np.trapz(f, t))

        half = F_peak / 2.0
        above_half = np.where(f >= half)[0]
        if len(above_half) >= 2:
            width = float(t[above_half[-1]] - t[above_half[0]])
        else: width = np.nan

        return T_peak, F_peak, width, area
    except:
        return np.nan, np.nan, np.nan, np.nan

def generate_plot_base64(user_t, user_f, species_name):
    try:
        plt.figure(figsize=(8, 5))

        # 1. User Data (Blue Line)
        plt.plot(user_t, user_f, label='Your Sample', color='#0066cc', linewidth=2)

        # 2. Reference Data (Red Line)
        if ref_df is not None and species_name != "Unknown":
            found = False
            cols = ref_df.columns.tolist()

            # Logic: ‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÉ‡∏ô‡∏´‡∏±‡∏ß‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå (Case Insensitive)
            # ‡∏ï‡∏±‡∏î‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤ 'sp' ‡∏≠‡∏≠‡∏Å‡∏ñ‡πâ‡∏≤‡∏°‡∏µ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏Å‡∏ß‡πâ‡∏≤‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
            clean_species = species_name.replace('sp', '').strip().lower()
            if not clean_species: clean_species = species_name.lower() # ‡∏Å‡∏±‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤‡∏á

            # ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡∏´‡∏≤‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå T ‡∏ó‡∏µ‡πà‡∏ä‡∏∑‡πà‡∏≠‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô
            target_t_col = None
            target_f_col = None

            print(f"üîç Searching graph for species: '{species_name}' (key: '{clean_species}')")

            for col in cols:
                col_lower = str(col).lower()
                # ‡∏ñ‡πâ‡∏≤‡πÄ‡∏à‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå AND ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏±‡πâ‡∏ô‡∏°‡∏µ‡∏ï‡∏±‡∏ß T ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏ä‡∏∑‡πà‡∏≠
                if clean_species in col_lower and 't' in col_lower:
                    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå T ‡∏à‡∏£‡∏¥‡∏á‡πÜ (‡∏°‡∏±‡∏Å‡∏à‡∏∞‡∏•‡∏á‡∏ó‡πâ‡∏≤‡∏¢‡∏î‡πâ‡∏ß‡∏¢‡πÄ‡∏•‡∏Ç ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô T ‡πÄ‡∏â‡∏¢‡πÜ)
                    # ‡πÄ‡∏ä‡πà‡∏ô "PtanalbaT1" -> T ‡∏≠‡∏¢‡∏π‡πà‡∏´‡∏ô‡πâ‡∏≤‡πÄ‡∏•‡∏Ç
                    if re.search(r't\d*$', col_lower):
                        # ‡∏´‡∏≤‡∏Ñ‡∏π‡πà F ‡∏Ç‡∏≠‡∏á‡∏°‡∏±‡∏ô
                        # ‡∏™‡∏°‡∏°‡∏ï‡∏¥ col = PtanalbaT1 -> ‡∏´‡∏≤ PtanalbaF1
                        # ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ: ‡πÅ‡∏ó‡∏ô‡∏ó‡∏µ‡πà T ‡∏ï‡∏±‡∏ß‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡πÄ‡∏õ‡πá‡∏ô F
                        prefix = str(col)[:str(col).lower().rfind('t')]
                        suffix = str(col)[str(col).lower().rfind('t')+1:]

                        # ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏£‡πà‡∏≤‡∏á‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠ col F
                        candidate_f = f"{prefix}F{suffix}" # ‡πÅ‡∏ö‡∏ö Case Sensitive (‡∏•‡∏≠‡∏á‡πÄ‡∏î‡∏≤)

                        # ‡∏ß‡∏ô‡∏´‡∏≤‡∏ä‡∏∑‡πà‡∏≠ F ‡∏ó‡∏µ‡πà‡∏ñ‡∏π‡∏Å‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ô cols ‡∏à‡∏£‡∏¥‡∏á‡πÜ
                        actual_f_col = None
                        for f_c in cols:
                            if f_c.lower() == candidate_f.lower():
                                actual_f_col = f_c
                                break

                        if actual_f_col:
                            target_t_col = col
                            target_f_col = actual_f_col
                            print(f"   ‚úÖ Found Match! T='{target_t_col}', F='{target_f_col}'")
                            break

            if target_t_col and target_f_col:
                ref_t = pd.to_numeric(ref_df[target_t_col], errors='coerce')
                ref_f = pd.to_numeric(ref_df[target_f_col], errors='coerce')

                mask = ~np.isnan(ref_t) & ~np.isnan(ref_f)
                ref_t, ref_f = ref_t[mask], ref_f[mask]

                sort_idx = np.argsort(ref_t)
                plt.plot(ref_t.iloc[sort_idx], ref_f.iloc[sort_idx],
                         label=f'Ref: {species_name}',
                         color='#ff3333', linestyle='--', linewidth=2, alpha=0.8)
                found = True
            else:
                print(f"   ‚ùå No matching columns found in PROJECT_DATA for {species_name}")

        plt.title(f"Comparison Result: {species_name}", fontsize=14)
        plt.xlabel("Temperature (¬∞C)")
        plt.ylabel("Fluorescence (Diff)")
        plt.legend()
        plt.grid(True, linestyle=':', alpha=0.6)
        plt.tight_layout()

        buf = io.BytesIO()
        plt.savefig(buf, format='png', dpi=100)
        plt.close()
        buf.seek(0)
        img_str = base64.b64encode(buf.read()).decode('utf-8')
        return f"data:image/png;base64,{img_str}"

    except Exception as e:
        print(f"Plot Error: {e}")
        plt.close()
        return None

# ==========================================
# 3. API ENDPOINTS
# ==========================================
@app.get("/")
def home():
    ref_status = "Loaded" if ref_df is not None else "Not Found"
    return {"message": f"Orchid AI Ready. Ref: {ref_status}"}

@app.post("/predict")
async def predict(files: List[UploadFile] = File(...)):
    if not model_data:
        raise HTTPException(status_code=500, detail="Model file not found.")

    all_results = []

    try:
        for file in files:
            contents = await file.read()
            filename = file.filename.lower()
            try:
                if filename.endswith('.csv'):
                    df = pd.read_csv(io.BytesIO(contents))
                else:
                    df = pd.read_excel(io.BytesIO(contents))
            except:
                continue

            columns = df.columns.tolist()
            processed_pairs = set()

            def process_and_predict(t_arr, f_arr, sample_name):
                T_peak, F_peak, width, area = extract_peak_features(t_arr, f_arr)
                if not np.isnan(T_peak):
                    features_df = pd.DataFrame([[T_peak, F_peak, width, area]],
                                             columns=["T_peak", "F_peak", "Width_FWHM", "Area"])

                    # ‚úÖ 1. ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏• (Predict Class)
                    pred_idx = model_data["model"].predict(features_df)[0]
                    species_name = model_data["label_encoder"].inverse_transform([pred_idx])[0]

                    # ‚úÖ 2. ‡∏´‡∏≤‡∏Ñ‡πà‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏°‡∏±‡πà‡∏ô‡πÉ‡∏à (Confidence Score)
                    probabilities = model_data["model"].predict_proba(features_df)[0]
                    confidence = round(probabilities[pred_idx] * 100, 2) # ‡πÅ‡∏õ‡∏•‡∏á‡πÄ‡∏õ‡πá‡∏ô %

                    # ‚úÖ 3. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü (‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏î‡∏á ‡∏ñ‡πâ‡∏≤‡∏´‡∏≤‡πÄ‡∏à‡∏≠)
                    plot_image = generate_plot_base64(t_arr, f_arr, species_name)

                    return {
                        "filename": file.filename,
                        "sample_id": sample_name,
                        "T_peak": round(T_peak, 4),
                        "F_peak": round(F_peak, 4),
                        "Width_FWHM": round(width, 4),
                        "Area": round(area, 4),
                        "predicted_species": species_name,
                        "confidence_score": f"{confidence}%", # ‡∏™‡πà‡∏á‡∏Ñ‡πà‡∏≤ % ‡∏Å‡∏•‡∏±‡∏ö‡πÑ‡∏õ
                        "plot_image": plot_image
                    }
                return None

            if 'T' in columns and 'F' in columns:
                res = process_and_predict(
                    pd.to_numeric(df['T'], errors='coerce').values,
                    pd.to_numeric(df['F'], errors='coerce').values,
                    "Uploaded-Sample"
                )
                if res: all_results.append(res)

            for col in columns:
                m = re.match(r"^(.*)T(\d+)$", str(col))
                if m:
                    prefix, num = m.group(1), m.group(2)
                    f_col = f"{prefix}F{num}"
                    if f_col in columns and col not in processed_pairs:
                        processed_pairs.add(col)
                        res = process_and_predict(
                            pd.to_numeric(df[col], errors='coerce').values,
                            pd.to_numeric(df[f_col], errors='coerce').values,
                            f"{prefix}-{num}"
                        )
                        if res: all_results.append(res)

        if not all_results:
             return {"success": False, "message": "No valid data found."}

        return {"success": True, "results": all_results}

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == '__main__':
    port = int(os.environ.get('PORT', 8000))
    uvicorn.run(app, host='0.0.0.0', port=port)