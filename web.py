# -*- coding: utf-8 -*-
"""web.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12w-zLfUrcKXiEDE-uvGpt7VtLst2_9e_
"""

from fastapi import FastAPI, UploadFile, File
from fastapi.middleware.cors import CORSMiddleware
import pandas as pd
import numpy as np
import joblib
import io
import traceback
import re
import os

app = FastAPI(
    title="Orchid Species Classifier API",
    description="Predict orchid species from thermal-fluorescence data using Decision Tree",
    version="1.0.0"
)

# Allow all origins (adjust for production)
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- 1. Load Model ---
MODEL_PATH = "orchid_decision_tree_v1.pkl"

if os.path.exists(MODEL_PATH):
    try:
        artifacts = joblib.load(MODEL_PATH)
        model = artifacts["model"]
        le = artifacts["label_encoder"]
        expected_features = artifacts["feature_columns"]
        class_labels = model.classes_
        print("‚úÖ Model loaded successfully.")
        print("Expected features:", expected_features)
    except Exception as e:
        print(f"‚ùå Model loading failed: {e}")
        model = None
        le = None
        expected_features = ["T_peak", "F_peak", "width", "area"]
else:
    print(f"‚ö†Ô∏è Model file not found: {MODEL_PATH}")
    model = None
    le = None
    expected_features = ["T_peak", "F_peak", "width", "area"]

@app.get("/")
def home():
    return {
        "message": "Orchid Species Prediction API is running!",
        "model_loaded": model is not None
    }

# --- 2. Feature Extraction (identical to Colab) ---
def extract_peak_features(t, f):
    mask = ~np.isnan(t) & ~np.isnan(f)
    t = np.asarray(t[mask])
    f = np.asarray(f[mask])
    if len(t) < 3:
        return np.nan, np.nan, np.nan, np.nan

    sort_idx = np.argsort(t)
    t = t[sort_idx]
    f = f[sort_idx]

    peak_idx = np.argmax(f)
    F_peak = float(f[peak_idx])
    T_peak = float(t[peak_idx])

    # Compatibility with numpy versions
    if hasattr(np, 'trapezoid'):
        area = float(np.trapezoid(f, x=t))
    else:
        area = float(np.trapz(f, t))

    half = F_peak / 2.0
    above_half = np.where(f >= half)[0]
    if len(above_half) >= 2:
        width = float(t[above_half[-1]] - t[above_half[0]])
    else:
        width = np.nan
    return T_peak, F_peak, width, area

# --- 3. T/F Pair Detection (identical to Colab) ---
def find_tf_pairs(columns):
    pairs = []
    for col in columns:
        m = re.match(r"^(.*)T(\d+)$", col)
        if m:
            prefix, rep = m.group(1), m.group(2)
            t_col = col
            f_col = f"{prefix}F{rep}"
            if f_col in columns:
                pairs.append((t_col, f_col))
    return pairs

# --- 4. Main Prediction Endpoint ---
@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    try:
        if model is None:
            return {"error": "Model not loaded", "results": []}

        # Read uploaded Excel file
        contents = await file.read()
        df = pd.read_excel(io.BytesIO(contents))
        print(f"üì• Received file with shape: {df.shape}")

        # Find T/F pairs (e.g., PtanalbaT1 & PtanalbaF1)
        tf_pairs = find_tf_pairs(df.columns.tolist())
        if not tf_pairs:
            return {"error": "No valid T/F column pairs found", "results": []}

        # Extract features from ALL pairs
        all_features = []
        for t_col, f_col in tf_pairs:
            t_vals = pd.to_numeric(df[t_col], errors='coerce').values
            f_vals = pd.to_numeric(df[f_col], errors='coerce').values
            T_peak, F_peak, width, area = extract_peak_features(t_vals, f_vals)
            if not np.isnan(T_peak):
                all_features.append([T_peak, F_peak, width, area])

        if not all_features:
            return {"error": "No valid features extracted", "results": []}

        # Convert to numpy array
        X_all = np.array(all_features)
        print(f"üß† Extracted {len(all_features)} samples for prediction")

        # Predict probabilities for all samples
        probs_all = model.predict_proba(X_all)

        # Average probabilities across all samples ‚Üí single prediction
        avg_probs = np.mean(probs_all, axis=0)

        # Get top 4 predictions
        top_indices = np.argsort(avg_probs)[::-1][:4]

        top_4_data = []
        max_score = 0
        for rank, idx in enumerate(top_indices, start=1):
            label_code = class_labels[idx]
            try:
                species_name = le.inverse_transform([label_code])[0]
            except Exception:
                species_name = "Unknown"

            confidence = int(round(avg_probs[idx] * 100))
            if rank == 1:
                max_score = confidence

            display_text = f"{rank}. {species_name} {confidence}%"
            top_4_data.append({
                "rank": rank,
                "species": display_text,
                "confidence": confidence
            })

        # Return single result (not grouped)
        result = {
            "filename": file.filename,
            "group_real_name": "Combined Prediction",
            "top_4_details": top_4_data,
            "sort_score": max_score
        }

        return {"results": [result]}

    except Exception as e:
        print("‚ùå Error during prediction:")
        print(traceback.format_exc())
        return {"error": str(e), "results": []}