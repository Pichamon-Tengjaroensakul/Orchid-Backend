# -*- coding: utf-8 -*-
"""new model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sLFS0SbED5SaWieGNJBGTfVLj6q6tdjp

‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train test
"""

import pandas as pd
import numpy as np
import re
import os
import zipfile
from sklearn.model_selection import train_test_split
from google.colab import files

# ==========================================
# 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• (‡πÅ‡∏Å‡πâ‡πÉ‡∏´‡πâ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á xlsx ‡πÅ‡∏•‡∏∞ csv)
# ==========================================
file_name = '/content/drive/MyDrive/Web Orchid/PROJECT_DATA.xlsx' # ‡∏´‡∏£‡∏∑‡∏≠‡∏ä‡∏∑‡πà‡∏≠‡πÑ‡∏ü‡∏•‡πå .xlsx ‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì

print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå: {file_name} ...")
try:
    if file_name.endswith('.csv'):
        df = pd.read_csv(file_name)
    else:
        df = pd.read_excel(file_name)
    print("‚úÖ ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à!")
except Exception as e:
    print(f"‚ùå ‡∏≠‡πà‡∏≤‡∏ô‡πÑ‡∏ü‡∏•‡πå‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {e}")
    df = pd.DataFrame()

# ==========================================
# 2. ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Feature (Peak, Area, Width)
# ==========================================
def get_species_name(col_name):
    m = re.match(r"^([A-Za-z]+)", str(col_name))
    if m: return m.group(1)
    return "Unknown"

def extract_peak_features(t, f):
    mask = ~np.isnan(t) & ~np.isnan(f)
    t, f = np.asarray(t[mask]), np.asarray(f[mask])

    if len(t) < 3: return np.nan, np.nan, np.nan, np.nan

    sort_idx = np.argsort(t)
    t, f = t[sort_idx], f[sort_idx]

    peak_idx = np.argmax(f)
    F_peak = float(f[peak_idx])
    T_peak = float(t[peak_idx])

    if hasattr(np, 'trapezoid'): area = float(np.trapezoid(f, x=t))
    else: area = float(np.trapz(f, t))

    half = F_peak / 2.0
    above_half = np.where(f >= half)[0]
    if len(above_half) >= 2:
        width = float(t[above_half[-1]] - t[above_half[0]])
    else: width = np.nan

    return T_peak, F_peak, width, area

# ==========================================
# 3. ‡∏ß‡∏ô‡∏•‡∏π‡∏õ‡∏™‡∏Å‡∏±‡∏î Feature ‡∏à‡∏≤‡∏Å‡∏ó‡∏∏‡∏Å‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå
# ==========================================
if not df.empty:
    columns = df.columns.tolist()
    data_rows = []
    processed_pairs = set()

    print("‚öôÔ∏è ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Feature ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î...")
    for col in columns:
        m = re.match(r"^(.*)T(\d+)$", str(col))
        if m:
            prefix, num = m.group(1), m.group(2)
            f_col = f"{prefix}F{num}"

            if f_col in columns and col not in processed_pairs:
                processed_pairs.add(col)

                t_vals = pd.to_numeric(df[col], errors='coerce').values
                f_vals = pd.to_numeric(df[f_col], errors='coerce').values

                # ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏ó‡∏±‡∏ô‡∏ó‡∏µ
                T_peak, F_peak, width, area = extract_peak_features(t_vals, f_vals)
                species = get_species_name(col)

                if not np.isnan(T_peak):
                    data_rows.append({
                        "species": species,
                        "T_peak": T_peak,
                        "F_peak": F_peak,
                        "Width_FWHM": width,
                        "Area": area
                    })

    full_df = pd.DataFrame(data_rows)
    print(f"‚úÖ ‡∏™‡∏Å‡∏±‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ: {len(full_df)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")

    # ==========================================
    # 4. Clean Data & Split (‡∏ï‡∏≤‡∏°‡πÄ‡∏á‡∏∑‡πà‡∏≠‡∏ô‡πÑ‡∏Ç‡πÄ‡∏î‡∏¥‡∏°)
    # ==========================================
    # ‡∏•‡∏ö‡πÅ‡∏ñ‡∏ß‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏ß‡πà‡∏≤‡∏á‡∏ó‡∏¥‡πâ‡∏á (‡πÑ‡∏°‡πà‡πÄ‡∏ï‡∏¥‡∏° Median)
    full_df.dropna(inplace=True)
    print(f"‚úÖ ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏û‡∏£‡πâ‡∏≠‡∏°‡πÉ‡∏ä‡πâ‡∏´‡∏•‡∏±‡∏á Clean: {len(full_df)} ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á")

    try:
        # ‡πÅ‡∏ö‡πà‡∏á 90% (Train+Val) : 10% (Test)
        train_val_df, test_df = train_test_split(full_df, test_size=0.10, stratify=full_df['species'], random_state=42)

        # ‡πÅ‡∏ö‡πà‡∏á 90% ‡∏ô‡∏±‡πâ‡∏ô‡πÄ‡∏õ‡πá‡∏ô Train ‡πÅ‡∏•‡∏∞ Validation
        train_final_df, validation_df = train_test_split(train_val_df, test_size=1/9, stratify=train_val_df['species'], random_state=42)

        # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏£‡∏ß‡∏°
        output_filename = 'orchid_dataset_ready.xlsx'
        with pd.ExcelWriter(output_filename) as writer:
            train_final_df.to_excel(writer, sheet_name='Train', index=False)
            validation_df.to_excel(writer, sheet_name='Validation', index=False)
            test_df.to_excel(writer, sheet_name='Test', index=False)
        print(f"üíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÑ‡∏ü‡∏•‡πå‡∏´‡∏•‡∏±‡∏Å: {output_filename}")

        # ==========================================
        # 5. ‡∏™‡∏£‡πâ‡∏≤‡∏á Blind Test 4 ‡∏ä‡∏∏‡∏î (‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ Feature ‡πÅ‡∏•‡πâ‡∏ß)
        # ==========================================
        print("üì¶ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå Blind Test...")
        test_shuffled = test_df.sample(frac=1, random_state=99)
        chunks = np.array_split(test_shuffled, 4)

        zip_filename = 'blind_tests_features.zip'
        with zipfile.ZipFile(zip_filename, 'w') as zipf:
            for i, chunk in enumerate(chunks):
                # ‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏à‡∏ó‡∏¢‡πå (‡∏•‡∏ö‡πÄ‡∏â‡∏•‡∏¢ species ‡∏≠‡∏≠‡∏Å ‡πÅ‡∏ï‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ T_peak, etc.)
                q_name = f'blind_test_set_{i+1}_question.xlsx'
                chunk.drop(columns=['species']).to_excel(q_name, index=False)
                zipf.write(q_name)
                os.remove(q_name)

                # ‡πÑ‡∏ü‡∏•‡πå‡πÄ‡∏â‡∏•‡∏¢ (‡∏°‡∏µ‡∏Ñ‡∏£‡∏ö)
                a_name = f'blind_test_set_{i+1}_answer.xlsx'
                chunk.to_excel(a_name, index=False)
                zipf.write(a_name)
                os.remove(a_name)

        files.download(output_filename)
        files.download(zip_filename)

    except ValueError as ve:
        print(f"‚ùå Error ‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•: {ve}")
else:
    print("‚ö†Ô∏è ‡πÑ‡∏°‡πà‡∏û‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•")

import pandas as pd
import numpy as np
import joblib
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report

# ==========================================
# 1. ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
# ==========================================
dataset_file = '/content/drive/MyDrive/Web Orchid/orchid_dataset_ready_l.xlsx'

print(f"üìÇ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å {dataset_file} ...")
try:
    train_df = pd.read_excel(dataset_file, sheet_name='Train')
    val_df = pd.read_excel(dataset_file, sheet_name='Validation')

    # Clean Data
    feature_cols = ["T_peak", "F_peak", "Width_FWHM", "Area"]
    train_df.dropna(subset=feature_cols, inplace=True)
    val_df.dropna(subset=feature_cols, inplace=True)

    X_train = train_df[feature_cols]
    y_train = train_df["species"]
    X_val = val_df[feature_cols]
    y_val = val_df["species"]

    # Encode Label
    le = LabelEncoder()
    y_train_encoded = le.fit_transform(y_train)
    y_val_encoded = le.transform(y_val)

    # ==========================================
    # 2. Grid Search (‡∏õ‡∏£‡∏±‡∏ö‡∏à‡∏π‡∏ô‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏Å‡πâ Overfitting)
    # ==========================================
    print("\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤ Decision Tree ‡∏ó‡∏µ‡πà‡∏™‡∏°‡∏î‡∏∏‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î...")

    param_grid = {
        'max_depth': [3, 4, 5, 6, 7, 8],
        'min_samples_leaf': [3, 4, 5, 6],
        'min_samples_split': [5, 10, 15],
        'criterion': ['gini', 'entropy'],
        'ccp_alpha': [0.0, 0.01, 0.02]
    }

    grid = GridSearchCV(DecisionTreeClassifier(random_state=42), param_grid, cv=5, scoring='accuracy')
    grid.fit(X_train, y_train_encoded)

    best_model = grid.best_estimator_

    print(f"‚úÖ ‡πÄ‡∏à‡∏≠‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÅ‡∏•‡πâ‡∏ß!")
    print(f"   - Best Params: {grid.best_params_}")
    print(f"   - Training Score (CV): {grid.best_score_:.2%}")

    # ==========================================
    # 3. ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡πÅ‡∏•‡∏∞‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å
    # ==========================================
    # ‡∏ß‡∏±‡∏î‡∏ú‡∏•‡∏Å‡∏±‡∏ö Validation Set
    val_preds = best_model.predict(X_val)
    val_acc = accuracy_score(y_val_encoded, val_preds)

    print("-" * 30)
    print(f"üìä Validation Accuracy: {val_acc:.2%}")
    print("-" * 30)

    # ‚ö†Ô∏è ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏à‡∏∏‡∏î‡∏ó‡∏µ‡πà Error: ‡πÄ‡∏û‡∏¥‡πà‡∏° labels=... ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Class
    all_labels = np.arange(len(le.classes_))
    print(classification_report(
        y_val_encoded,
        val_preds,
        labels=all_labels,          # ‡∏ö‡∏±‡∏á‡∏Ñ‡∏±‡∏ö‡πÉ‡∏´‡πâ‡∏î‡∏π‡∏Ñ‡∏£‡∏ö‡∏ó‡∏∏‡∏Å Index
        target_names=le.classes_,   # ‡∏ä‡∏∑‡πà‡∏≠‡∏™‡∏≤‡∏¢‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡∏Ñ‡∏£‡∏ö 13 ‡∏ä‡∏∑‡πà‡∏≠
        zero_division=0             # ‡∏ñ‡πâ‡∏≤‡∏û‡∏±‡∏ô‡∏ò‡∏∏‡πå‡πÑ‡∏´‡∏ô‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÉ‡∏´‡πâ‡πÉ‡∏™‡πà 0 ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á Error
    ))

    # ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
    model_filename = "orchid_model_final.pkl"
    joblib.dump({
        "model": best_model,
        "label_encoder": le,
        "feature_columns": feature_cols
    }, model_filename)

    print(f"\nüíæ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏•‡πâ‡∏ß: {model_filename}")


except Exception as e:
    print(f"‚ùå Error: {e}")